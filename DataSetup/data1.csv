Link,Name,Description,Tags,Data Format,Data size (mb),Number of Rows,Date posted,Language
https://huggingface.co/datasets/Anthropic/hh-rlhf,"HH RLHF (Anthropic)","The ""Anthropic/hh-rlhf"" dataset contains human feedback data to aid reinforcement learning from human feedback (RLHF). It consists of two subsets: (1) preference data on helpfulness and harmlessness to train reward models, and (2) red teaming dialogues that test AI model safety. The dataset warns against training dialogue agents on this data to prevent harmful behavior. It is provided in JSONL format with detailed annotations on human interactions and red team attempts. It helps fine-tune models for safety and alignment purposes. Visit the link provided or this paper for more information https://arxiv.org/abs/2204.05862.","RLHF, Helpfulness, Harmless, Science",accepted/rejected,79.3,"169,352",08/12/2022,English
https://huggingface.co/datasets/berkeley-nest/Nectar,'Nectar (Berkeley NEST)',"The Nectar dataset is a high-quality human feedback dataset developed at Berkeley. It features multi-model comparisons across GPT-4, GPT-3.5-turbo, LLama, and others, producing over 3.8M pairwise rankings. It focuses on reinforcement learning from human feedback (RLHF), measuring helpfulness and harmlessness. Prompts are sourced from multiple datasets, and responses undergo detailed GPT-4-based ranking. The dataset is licensed under Apache-2.0 but restricts competition with OpenAI.","RLHF, Helpfulness, Harmless, Educational, Comparison",accepted/rejected,517,"182,954",13/10/2023,English
https://huggingface.co/datasets/Intel/orca_dpo_pairs,'ORCA DPO Pairs (Intel)',"The ""Intel/orca_dpo_pairs"" dataset is designed for instruction tuning and reward model training. It contains pairs of responses, with one chosen and the other rejected, across various scenarios. The dataset provides structured challenges for natural language understanding, focusing on tasks such as generating RDF triplets from text or selecting the best response to queries. This dataset aids in evaluating how well models align with task requirements and user preferences.","TOOL/instruction, DPO, NLG",accepted/rejected,35.3,"12,859",21/09/2023,English
https://huggingface.co/datasets/openai/webgpt_comparisons?row=0,'WebGPT Comparisons (OpenAI)',"The WebGPT Comparisons dataset contains 19,578 pairs of model-generated answers to questions, along with human preference scores. It was designed to train reward models for aligning AI behavior with human feedback. Each entry includes question text, answer pairs, associated quotes, tokenized content, and numerical preference scores. These comparisons played a key role in developing long-form question-answering systems.","RLHF, Helpfulness, Educational, News",Scored,279,"19,578",18/12/2022,English
https://huggingface.co/datasets/nvidia/HelpSteer,'Help Steer (NVIDIA)',"HelpSteer is an open-source dataset designed to train models for helpful, factually correct, and coherent responses, allowing flexibility in response complexity and verbosity. It contains 37,120 annotated samples, each rated across five attributes: helpfulness, correctness, coherence, complexity, and verbosity. Developed by NVIDIA, it supports tasks like rewriting, summarization, and question answering, with annotation handled by Scale AI.","Helpfulness, NLG, Medical, Educational",Scored on multiple categories ,16.1,"37,120",15/11/2023,English
https://huggingface.co/datasets/dikw/hh_rlhf_cn,'HH RLHF CN (DIKW)',"The ""hh_rlhf_cn"" dataset is a Chinese-translated version of the helpful and harmless data from Anthropic's RLHF paper. It offers 170,000 cleaned training entries and 9,000 test entries, along with additional subsets for harmless and helpful responses. The dataset aims to support model training for alignment research in Chinese.","RLHF, Helpfulness, Harmless, Chinese, Translation",accepted/rejected,369,"362,909",24/08/2023,Chinese
https://www.kaggle.com/datasets/pallaviroyal/dataset-to-evaluate-language-models,'Dataset to Evaluate Language Models (Kaggle)',"The Kaggle dataset titled ""Dataset to Evaluate Language Models"" focuses on testing language models for truthfulness, bias, and toxicity. It includes datasets like CrowS-Pairs, WinoGender, and TruthfulQA, which measure biases across multiple categories, coreference resolution impacts, and a model's ability to identify literal truths. These datasets aim to ensure that language models align with real-world accuracy and fairness.","Bias, Truthfulness, NLG, Fairness",Accepted/rejected,0.443,817,28/02/2023,English
https://huggingface.co/datasets/Unified-Language-Model-Alignment/Anthropic_HH_Golden,'Anthropic HH Golden (Unified Language Model Alignment)',"The ""Anthropic_HH_Golden"" dataset extends Anthropic’s Helpful and Harmless (HH) dataset by replacing positive responses with GPT-4-generated, higher-quality responses. It aims to improve model alignment techniques like RLHF, DPO, and ULMA by treating positive and negative samples differently. This dataset demonstrates how using refined positive samples boosts alignment performance.","RLHF, Helpfulness, DPO, Safety",Accepted/rejected,63.2,"44,849",04/10/2023,English
https://huggingface.co/datasets/tatsu-lab/alpaca,'Alpaca (Tatsu Lab)',"The Alpaca dataset contains 52,000 instruction-following examples generated by OpenAI's text-davinci-003. It is used to fine-tune language models to better follow instructions. This data was developed with a simplified pipeline to reduce costs and improve output diversity. The dataset supports tasks such as classification and question answering, promoting reproducible research for language model alignment. ","TOOL/instruction, NLG, Reproducibility",Classification,24.2,"52,002",13/03/2023,English
https://huggingface.co/datasets/yitingxie/rlhf-reward-datasets?row=2,'RLHF Reward Datasets (Yiting Xie)',"The ""RLHF Reward Datasets"" on Hugging Face includes a collection of human-annotated interactions to evaluate and fine-tune reward models. It contains dialogue exchanges, task prompts, and responses with labeled outcomes, enabling better alignment of models to human preferences. This dataset supports developing systems that offer more accurate and helpful answers to user queries.","RLHF, Dialogue, Task Prompts",Accepted/rejected,57.3,"81,359",01/01/2023,English
https://huggingface.co/datasets/stanfordnlp/SHP?row=1,'SHP (Stanford NLP)',"The Stanford Human Preferences (SHP) dataset contains 385K collective human preferences across Reddit responses, covering 18 domains such as cooking and legal advice. It leverages Reddit post timestamps to infer user preferences and focuses on evaluating model helpfulness. The dataset aids in training reward models for RLHF and NLG evaluation.","RLHF, NLG, Reddit, Evaluation",Scored ,827,"385,563",19/02/2023,English
https://huggingface.co/datasets/interstellarninja/tool-calls-dpo,'Tool Calls DPO (Interstellar Ninja)',"The ""tool-calls-dpo"" dataset provides examples of function-calling interactions between AI models and users, designed to enhance the model's ability to follow instructions precisely. It contains XML-tagged function signatures, prompts, and responses to demonstrate how models handle structured tasks, such as searching books or generating barcodes. This dataset supports the development of systems that align well with user intents by training models on complex decision-making scenarios.","DPO, TOOL/instruction, NLG, Structured Tasks",Accepted/rejected,0.157,235,23/01/2024,English
https://huggingface.co/datasets/abacusai/MetaMath_DPO_FewShot,'MetaMath DPO Few Shot (Abacus.AI)',"The ""MetaMath_DPO_FewShot"" dataset extends the GSM8K math dataset with data augmentation to improve LLM precision in mathematical reasoning. It provides correct and subtly corrupted answers to math problems, focusing on aligning models toward accurate intermediate calculations. The dataset consists of 393,999 training examples and 1,000 evaluation examples, supporting fine-tuning of language models using the DPOP loss function.","DPO, Math, Educational","Accepted/rejected, Math",561,"394,999",26/02/2023,English
https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF,'PKU Safe RLHF (PKU Alignment)',"The PKU-SafeRLHF dataset contains 83.4K preference entries, focusing on evaluating responses based on harmlessness and helpfulness. It categorizes harmful content across 19 categories and assigns severity levels to harmful events. This dataset helps in training models to balance safety with effective responses.","RLHF, Safety, Helpfulness, Harmless",Scored on multiple categories ,465,"164,236",10/09/2024,English
https://huggingface.co/datasets/neovalle/H4rmony,'Harmony (Neovalle)',"The H4rmony dataset focuses on integrating ecolinguistic principles into large language models (LLMs), promoting environmental awareness and sustainability. It contains prompts with ranked responses based on their alignment with ecolinguistic values. Responses are generated to reflect different perspectives, helping fine-tune AI systems through Reinforcement Learning via Role-playing and Human Verification (RLRHV). This ongoing project bridges AI and environmental awareness, supporting alignment with sustainability goals.","EcoFriendly, Sustainability, Alignment, NLG",Accepted/rejected,0.81,"2,016",17/01/2024,English
https://huggingface.co/datasets/neovalle/H4rmony_dpo,'Harmony DPO (Neovalle)',"The H4rmony_DPO dataset is a streamlined version of the original H4rmony dataset, optimized for use with the DPOTrainer from the trl library. It supports fine-tuning language models to align with ecolinguistic principles, reinforcing environmentally conscious responses. This dataset is designed for reinforcement learning with a focus on environmental and sustainability values.","EcoFriendly, DPO, Sustainability",Accepted/rejected,0.566,"2,016",19/01/2024,English
https://huggingface.co/datasets/nz/highest-number-rlhf,'Highest Number RLHF (NZ)',"The ""highest-number-rlhf"" dataset helps train AI to compare quantities, such as length, population, or size, by providing pairs of items and asking which one is larger or smaller. It enhances AI's ability to reason and make quantitative comparisons, making it valuable for models requiring numerical understanding. ","RLHF, Math, Quantitative Reasoning",Accepted/rejected,9.45,"84,000",07/02/2024,English
https://huggingface.co/datasets/nz/anthropic-hh-golden-rlhf,'Anthropic HH Golden RLHF (NZ)',"The ""anthropic-hh-golden-rlhf"" dataset employs direct preference optimization to align AI behavior towards harmlessness, especially in scenarios involving questionable queries. It refines responses to prioritize safety while still being informative. This dataset supports reinforcement learning from human feedback (RLHF), ensuring that the AI's decision-making remains ethical and non-harmful.","RLHF, Safety, Harmless, Alignment",Accepted/rejected,22.9,"44,849",02/02/2024,English
https://huggingface.co/datasets/databricks/databricks-dolly-15k,'Databricks Dolly 15K (Databricks)',"The Databricks Dolly 15K dataset is an open-source collection of 15,000 instruction-following records created by Databricks employees. It covers various behavioral categories such as brainstorming, classification, closed and open QA, summarization, generation, and information extraction. This dataset aims to support the development of models following the approach outlined in the InstructGPT paper.","TOOL/instruction, NLG, Summarization",Classification,13.1,"15,011",30/06/2023,English
https://github.com/openai/prm800k,'PRM 800K (OpenAI)',"The PRM800K dataset focuses on process supervision, featuring 800,000 step-level correctness labels for AI-generated solutions to math problems from the MATH dataset. It aims to improve step-by-step reasoning in models by identifying errors within intermediate calculations. The dataset includes raw labels, labeler instructions, and evaluation tools for grading solutions. It is part of research towards enhancing model transparency and performance through process supervision techniques.","Math, Educational, Transparency, Process Supervision",Scored,,,30/05/2023,English
https://openaipublic.blob.core.windows.net/summarize-from-feedback/website/index.html#/,'Summarize from Feedback (OpenAI)',This collection features user posts from Reddit alongside model-generated summaries. The dataset emphasizes models' capability to condense long posts. Each entry contains the original post and multiple summaries from different models to explore differences in summarization quality.,"NLG, Summarization, Reddit",Multiple responses scored,,,02/09/2020,English
https://openaipublic.blob.core.windows.net/summarize-from-feedback/website/index.html#/tldr_comparisons,'TLDR Comparisons (OpenAI)',"This dataset presents comparisons between two summaries generated by the models for the same Reddit post. Human labelers select their preferred summaries and provide interpretation notes, which offer insights into the decision-making process based only on the summaries without seeing the original post.","NLG, Summarization, Reddit",Accepted/rejected,,,02/09/2020,English
https://openaipublic.blob.core.windows.net/summarize-from-feedback/website/index.html#/tldr_axis_evals,'TLDR Axis Evaluations (OpenAI)',"Focused on evaluating quality, this set assigns ratings (1-7) across several quality dimensions to each summary. Labelers assess the models' summaries based on clarity, accuracy, and other criteria, offering detailed feedback through interpretation notes after reviewing each summary.","NLG, Summarization, Reddit",Multiple responses scored,,,02/09/2020,English
https://openaipublic.blob.core.windows.net/summarize-from-feedback/website/index.html#/cnndm,'CNN/DM (OpenAI)',"This dataset includes news articles from CNN and DailyMail, paired with model-generated summaries. It serves as a benchmark for summarization performance in the context of news, exploring the challenges of extracting concise and relevant information from long-form journalism.","News, NLG, Summarization",Multiple responses scored,,,02/09/2020,English
https://openaipublic.blob.core.windows.net/summarize-from-feedback/website/index.html#/cnndm_axis_evals,'CNN/DM Axis Evaluations (OpenAI)',"Here, summaries from CNN/DM articles are scored across multiple axes, following the same 1-7 rating scale used in TL;DR evaluations. Labelers provide feedback on individual summaries to highlight strengths and areas for improvement, helping fine-tune models for summarizing news content.","News, NLG, Summarization",Multiple responses scored,,,02/09/2020,English
https://sites.google.com/view/mediqa2019,'MEDIQA 2019 (Google Sites)',"The MEDIQA 2019 dataset focuses on medical NLP tasks, including Natural Language Inference (NLI), Recognizing Question Entailment (RQE), and Question Answering (QA). It provides annotated datasets to develop and evaluate systems for clinical text processing. NLI contains 14,049 sentence pairs from MedNLI, RQE offers 8,588 medical question pairs, and QA includes medical questions with curated answer rankings from CHiQA. Evaluation metrics include accuracy, precision, and rank correlation.","Medical, NLG, QA, NLP",Multiple responses scored,,"14,049",15/04/2019,English
https://huggingface.co/datasets/chillies/IELTS_essay_human_feedback?row=0,'IELTS Essay Human Feedback (Chillies)',"The IELTS Essay Human Feedback dataset on Hugging Face provides prompts, essays, and human-scored feedback based on IELTS writing guidelines. It assesses essays on key criteria such as task achievement, coherence, cohesion, vocabulary, and grammar. The dataset contains examples of AI-generated and human responses, with scores and feedback for each. This resource is intended to train AI models in providing constructive essay critique aligned with IELTS standards, focusing on clarity, structure, and relevance.","Educational, Writing, Human Feedback",Accepted/rejected,13.8,"1,762",10/03/2024,English
https://huggingface.co/datasets/mair-lab/lave-human-feedback#lave-human-judgments,'LAVE Human Feedback (MAIR Lab)',"The LAVE Human Judgments dataset on Hugging Face contains human-evaluated answers for Visual Question Answering (VQA) tasks. It features outputs from models like BLIP2 and BLIP VQA across datasets such as VQAv2, VGQA, and OKVQA. Human scores reflect answer accuracy, ranging from 0 (incorrect) to 1 (correct), with 0.5 indicating ambiguous or incomplete responses. This dataset helps improve AI-generated VQA responses through better evaluation techniques.","Visual QA, NLG, Accuracy",Scored,1.62,"29,050",11/03/2024,English
https://huggingface.co/datasets/lmsys/lmsys-arena-human-preference-55k,'LMSYS Arena Human Preference 55K (LMSYS)',"The LMSYS Arena Human Preference 55K dataset contains over 55,000 samples for evaluating user preferences in chatbot ""battles."" Each sample includes responses from two different language models (LLMs) to the same question, with labels indicating which response was preferred (or if the match was a tie). The dataset covers interactions from 70+ state-of-the-art LLMs, such as GPT-4, Claude 2, and Llama 2, and supports the development of models by providing insights into user preferences.","RLHF, Dialogue, Evaluation",Accepted/rejected,184,"57,477",02/05/2024,English
https://huggingface.co/datasets/allenai/ai2_arc,'AI2 ARC (Allen AI)',"The AI2 ARC dataset contains 7,787 multiple-choice, grade-school-level science questions aimed at fostering research in advanced question answering. It is divided into two sets: the Challenge Set (harder questions missed by baseline models) and the Easy Set. The dataset also provides over 14 million science-related sentences and supports models with neural baseline implementations. Challenge Set: 1,119 train, 299 validation, 1,172 test samples. Easy Set: 2,251 train, 570 validation, 2,376 test samples.","Science, Educational, QA, NLG",Classification,1.21,"7,787",21/12/2023,English
https://huggingface.co/datasets/Dahoas/synthetic-instruct-gptj-pairwise,'Synthetic Instruct GPT-J Pairwise (Dahoas)',"The Synthetic Instruct-GPTJ Pairwise dataset contains synthetic prompt-response pairs, focusing on improving LLM performance by ranking generated responses. Each example includes a prompt and two completions: a chosen response and a rejected one. This structure helps models learn nuanced preferences between responses, essential for tasks like reinforcement learning through human feedback (RLHF).","RLHF, TOOL/instruction, NLG",Accepted/rejected,18.2,"33,143",09/01/2023,English
https://huggingface.co/datasets/lmsys/chatbot_arena_conversations,'Chatbot Arena Conversations (LMSYS)',"The Chatbot Arena Conversations Dataset consists of 33,000 cleaned conversations gathered from 13,000 unique users between April and June 2023. It compares responses from 20 large language models (LLMs) through pairwise human preferences. Key features include user votes, conversation metadata, OpenAI moderation tags, and toxic tags. The dataset offers insights into real-world prompts, model evaluation, safety, and content moderation, with contributions from models like GPT-4 and Claude-v1. It supports research in AI safety and model performance.","RLHF, Dialogue, Safety",Classification ,,"33,000",,English
https://huggingface.co/datasets/allenai/real-toxicity-prompts,'Real Toxicity Prompts (Allen AI)',"The RealToxicityPrompts dataset from AllenAI contains 100,000 sentence snippets sourced from the web, aiming to help researchers address toxic language generation risks in neural models. It provides prompts and their potential toxic continuations, helping evaluate the models' behavior in unsafe scenarios. This dataset is valuable for AI research focused on bias mitigation, safety, and toxic degeneration in NLP systems.","Safety, Harmless, Toxicity, NLP",Prompt and response,67.7,"99,442",17/08/2022,English
https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM,'GPT-4 LLM (Instruction Tuning with GPT-4)',"The GPT-4-LLM Repository provides datasets for building instruction-following language models using supervised and reinforcement learning. It includes 52K English and Chinese instruction-following samples, comparison data for reward models, and 9K responses to unnatural prompts. Fine-tuning was conducted on LLaMA models, demonstrating improved performance using GPT-4-generated data. This project aims to advance instruction-tuned LLMs and provides tools for reproducibility, including code and plotting scripts. Usage is restricted to non-commercial research.","Instruction, Chinese, NLG, Multilingual",Prompt and response,155,52000,17/04/2023,English
https://huggingface.co/datasets/Hello-SimpleAI/HC3,'HC3 (Hello Simple AI)',"The Databricks Dolly 15K dataset is an open-source collection of 15,000 instruction-following records created by Databricks employees. It includes diverse tasks such as closed QA, open QA, brainstorming, classification, summarization, and more. This dataset follows the methodology outlined in the InstructGPT paper, aiming to support the development of models capable of natural language instruction following.","Instruction, NLG, QA",Prompt and response,147,"48,644",18/01/2023,English
https://huggingface.co/datasets/PJMixers/tatsu-lab_alpaca_farm_human_preference-PreferenceShareGPT?row=4,'Alpaca Farm Human Preference (PJMixers)',"The Tatsu-Lab Alpaca Farm Human Preference (PreferenceShareGPT) dataset is designed for Reinforcement Learning from Human Feedback (RLHF) to improve model outputs by selecting the better response between two generated ones. It contains conversation data with human-written prompts and two responses, where a preference is provided for one of the responses. This dataset focuses on enforcing correctness and quality in executing prompts.","RLHF, Dialogue, Evaluation",Accepted/rejected,2.45,"3,798",28/05/2024,English
https://huggingface.co/datasets/google-research-datasets/go_emotions,'Go Emotions (Google Research)',"The GoEmotions dataset contains 58,000 Reddit comments labeled across 27 emotion categories or marked as neutral. It offers both raw data and a simplified version with predefined training, validation, and test splits. This dataset is widely used for research on sentiment analysis and emotion detection, aiming to improve the emotional understanding of language models.","Reddit, Sentiment Analysis, NLP",Classification ,28.3,"265,488",04/01/2024,English
https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset,'RLAIF-V Dataset (OpenBMB)',"The RLAIF-V-Dataset is a large-scale multimodal dataset designed to train models handling both visual and textual inputs, focusing on improving trustworthiness through AI feedback. It includes 83,132 preference pairs collected from diverse datasets like MSCOCO, VQA v2, and MovieNet, with tasks ranging from image captioning to question answering. This dataset is used to enhance models like MiniCPM-V and RLAIF-V, achieving GPT-4V-level performance in multimodal tasks.","Visual, Text, Multimodal, QA",Accepted/rejected,"4,850","33,835",19/05/2024,English
https://huggingface.co/datasets/alvarobartt/HelpSteer-AIF,'Help Steer AIF (Alvaro Bartt)',"The HelpSteer-AIF dataset provides a subset of 1,000 samples from a larger 37,120-entry Helpfulness dataset. It contains prompts and responses generated by an in-house 43B parameter LLM, focusing on tasks like classification, summarization, and brainstorming. Annotations were created using GPT-4 via distilabel, rating responses based on helpfulness, correctness, coherence, complexity, and verbosity. This dataset aims to align models to become more helpful and accurate while balancing response detail and complexity.","Helpfulness, Medical, Classification",Scored on multiple categories,0.667,"1,000",28/11/2023,English
https://huggingface.co/datasets/davanstrien/magpie-preference?row=24,'Magpie Preference (Davan Strien)',"The Magpie Preference Dataset contains human feedback on synthetic instruction-response pairs generated using the Magpie method. Users interact via a Gradio interface to generate prompts and responses, then provide binary feedback (thumbs up/down). This dataset supports preference learning for LLMs and is continuously updated. Each entry includes the timestamp, generated prompt, response, user preference, and session ID. The data aims to align models to follow instructions more effectively and ensure high response quality.","Human Feedback, NLG, Preference Learning",Accepted/rejected,0.986,476,02/10/2024,English
https://huggingface.co/datasets/danilopeixoto/pandora-rlhf,'Pandora RLHF (Danilo Peixoto)',"The Pandora RLHF dataset is designed for fine-tuning the Pandora Large Language Model using Direct Preference Optimization (DPO). It is built upon the Anthropic HH-RLHF dataset, providing conversational examples with user-assistant interactions. The dataset includes real-world scenarios, with assistants responding to prompts and guiding users, enabling reinforcement learning from human feedback. It is licensed under BSD-3-Clause and aims to enhance LLMs in alignment, helpfulness, and factual correctness.","RLHF, Dialogue, Safety",Accepted/rejected,176,"168,918",01/03/2024,English
https://huggingface.co/datasets/HuggingFaceH4/no_robots,'No Robots (HuggingFaceH4)',"The No Robots dataset consists of 10,000 instructions and demonstrations crafted by human annotators, intended for supervised fine-tuning (SFT) of language models. It covers categories like generation, QA, chat, and coding to enhance model alignment with human-like instructions. Each example features prompts and multi-turn conversations with distinct roles (system, user, assistant). It offers train and test splits of 9,500 and 500 examples, respectively. The dataset is licensed under CC BY-NC 4.0.","Instruction, NLG, QA, Dialogue",Prompt and response,11,"10,000",18/04/2024,English
https://huggingface.co/datasets/erhwenkuo/hh_rlhf-chinese-zhtw,'HH RLHF Chinese ZH-TW (Erhwenkuo)',"The hh_rlhf-chinese-zhtw dataset contains preference data for training reward models in Reinforcement Learning from Human Feedback (RLHF). It is intended to align models to be helpful and harmless, focusing on human preferences. The dataset includes both training and test sets in Chinese and English, with translations using OpenCC. Notably, it is not recommended for training conversational agents, as this may yield harmful behaviors. For research purposes only, it contains sensitive content related to harmful scenarios.","RLHF, Chinese, Helpfulness, Harmless",Accepted/rejected,179,"344,317",04/10/2023,Chinese
https://huggingface.co/datasets/amitagh/marathi-hh-rlhf-v02,'Marathi HH RLHF V02 (Amitagh)',"The Marathi HH-RLHF-v02 dataset is a translated subset of the Helpful and Harmless RLHF dataset, focusing on Marathi. It provides both original English and Marathi translations for conversations used to train preference or reward models for fine-tuning language models. This dataset is intended for RLHF training, not supervised training for dialogue agents, as misuse could lead to harmful behaviors.","RLHF, Multilingual, Helpfulness, Harmless",Accepted/rejected,42.2,"28,135",21/05/2024,Marathi
https://huggingface.co/datasets/PJMixers/ProlificAI_social-reasoning-rlhf-PreferenceShareGPT,'ProlificAI Social Reasoning RLHF (PJMixers)',"The ProlificAI Social Reasoning RLHF PreferenceShareGPT dataset contains conversations where users compare responses from multiple GPT models. It focuses on handling nuanced social dilemmas and human preferences, supporting RLHF training for large language models. Each entry includes human-provided prompts, multiple GPT-generated responses, and user preferences indicating which response is preferred for alignment purposes.","RLHF, Dialogue, Social Reasoning",Accepted/rejected,4.99,"3,820",28/05/2024,English
https://huggingface.co/datasets/PJMixers/trl-internal-testing_hh-rlhf-trl-style-PreferenceShareGPT,'TRL Internal Testing HH RLHF (PJMixers)',"The trl-internal-testing_hh-rlhf-trl-style-PreferenceShareGPT dataset contains conversations between humans and GPT models. It highlights user interactions and compares responses from multiple models to assess preference alignment. The data includes user prompts, GPT responses, and labels indicating which response aligns better with the intended style or goal. This dataset supports reinforcement learning from human feedback (RLHF) and model fine-tuning.","RLHF, Evaluation, Dialogue",Accepted/rejected,261,"169,027",29/05/2024,English
https://huggingface.co/datasets/PJMixers/tasksource_oasst2_pairwise_rlhf_reward-PreferenceShareGPT,'Tasksource OASST2 Pairwise RLHF Reward (PJMixers)',"The tasksource_oasst2_pairwise_rlhf_reward-PreferenceShareGPT dataset contains multilingual conversations between users and GPT models, focusing on instruction-following and response quality. Each data instance records a conversation, and two model responses—chosen and rejected—are labeled to support preference-based learning. This dataset aids reinforcement learning from human feedback (RLHF) to fine-tune model behavior across various tasks.","RLHF, Dialogue, Multilingual",Accepted/rejected,100,"28,379",29/05/2024,English
https://huggingface.co/datasets/PJMixers/samhog_psychology-RLHF-PreferenceShareGPT,'Samhog Psychology RLHF (PJMixers)',"The samhog_psychology-RLHF-PreferenceShareGPT dataset focuses on human-like conversations with psychological themes, collected for training and fine-tuning reinforcement learning models. It contains user-GPT dialogues involving common psychological issues such as anxiety, relationship challenges, and stress management, with multiple responses labeled as ""chosen"" or ""rejected"" to support preference-based training. This dataset aims to align language models with appropriate responses in sensitive scenarios.","RLHF, Dialogue, Psychology, Mental Health",Accepted/rejected,2.04,"2,000",29/05/2024,English
https://huggingface.co/datasets/kunishou/hh-rlhf-49k-ja,'HH RLHF 49K JA (Kunishou)',"The hh-rlhf-49k-ja dataset is a Japanese-translated version of part of the ""Anthropic/hh-rlhf"" dataset. It aims to support training with Reinforcement Learning from Human Feedback (RLHF) in Japanese. A flag called ng_translation is included to indicate unsuccessful translations. Users can filter out such entries to ensure data quality. This dataset is also integrated into the mosaicml/dolly_hhrlhf collection, enabling broader usage in fine-tuning models.","RLHF, Japanese, Harmless, Translation",Accepted/rejected,69.6,"49,424",18/05/2023,Japanese
https://huggingface.co/datasets/d0rj/rlhf-reward-datasets-ru,'RLHF Reward Datasets RU (D0rj)',"The RLHF Reward Datasets RU dataset provides prompts, responses, and human feedback in Russian. It is designed to fine-tune reward models through reinforcement learning from human feedback (RLHF). Each entry includes a prompt, chosen response, and rejected response, allowing models to learn by comparing preferred and non-preferred outputs. This dataset is helpful for improving conversational agents in Russian, ensuring better alignment with user preferences.","RLHF, Russian, Dialogue",Accepted/rejected,78.9,"81,359",05/06/2023,Russian
https://huggingface.co/datasets/germank/hh-rlhf_with_features_flan_t5_large,'HH RLHF with Features FLAN T5 Large (Germank)',"The HH-RLHF with Features (Flan-T5-Large) dataset is designed for fine-tuning and evaluating models through human preferences. It contains chosen and rejected responses along with multiple evaluation metrics such as helpfulness, intent, factuality, relevance, readability, and more. The dataset is useful for reward modeling and reinforcement learning from human feedback (RLHF), focusing on improving model alignment by comparing outputs based on these metrics.","RLHF, NLG, Evaluation",Accepted/rejected,15.7,"19,148",,English
https://huggingface.co/datasets/jojo0217/korean_rlhf_dataset,'Korean RLHF Dataset (Jojo0217)',"This dataset is part of efforts to enhance large language models (LLMs) by providing high-quality Korean-language data focused on Reinforcement Learning from Human Feedback (RLHF). The dataset consists of instruction-response pairs aligned to foster better understanding and alignment in Korean-language contexts. Notably, the data draws from existing sources like Stanford Alpaca and OpenAssistant, and was further refined using ChatGPT-3.5 Turbo. The dataset aims to support various applications, including question answering, summarization, and conversation generation, improving LLMs' alignment to human expectations within the Korean language. The dataset supports alignment training by leveraging structured prompts and responses, providing insights into interactions, including preferences for handling diverse inputs. It serves as a key resource for projects aiming to fine-tune models like Korean GPT-style assistants.","RLHF, Korean, Instruction, Multilingual",Prompt and response,80.1,"107,172",25/09/2023,Korean
https://huggingface.co/datasets/FreedomIntelligence/Arabic-preference-data-RLHF,'Arabic Preference Data RLHF (FreedomIntelligence)',"The Arabic Preference Data (RLHF) is a dataset focused on preference modeling, containing instructions along with both chosen and rejected responses in Arabic. It helps train models through Reinforcement Learning from Human Feedback (RLHF) by evaluating how well responses align with user needs. The dataset covers various topics, ranging from technical to social inquiries, and supports the development of language models tailored for Arabic-speaking users.","RLHF, Arabic, Dialogue",Accepted/rejected,34.4,"11,548",10/10/2023,Arabic
https://huggingface.co/datasets/qgallouedec/lm-human-preferences-descriptiveness,'LM Human Preferences Descriptiveness (QGallouedec)',"The LM Human Preferences Descriptiveness dataset, hosted on Hugging Face, contains human preferences for text output with an emphasis on how descriptive the language is. It is structured to compare two text responses—one ""chosen"" and one ""rejected""—based on human preferences for descriptiveness. This dataset is beneficial for fine-tuning language models to align more closely with human-like writing preferences, particularly those emphasizing vivid descriptions and coherent storytelling.","RLHF, NLG, Human Feedback",Accepted/rejected,1.61,"6,259",06/07/2023,English
https://huggingface.co/datasets/seeyssimon/sujet-finance-instruct-human-preference-13k,'Subject Finance Human Preference 13K (Seeyssimon)',"The Sujet Finance Instruct Human Preference dataset is designed to compare responses from two financial models, ""gemini-pro"" and ""bt4103_gguf_finance,"" to enhance their alignment with human preferences in financial contexts. Each entry in the dataset presents a financial question or task (prompt) and the corresponding responses from both models. Evaluators then select a preferred response, with outcomes marked as either a preference for Model A (gemini-pro), Model B (bt4103_gguf_finance), or a tie if neither response is favored. The dataset aims to improve financial consulting capabilities by training models to provide high-quality responses in areas such as investment advice, retirement planning, and sentiment analysis. It supports the fine-tuning of RLHF (Reinforcement Learning with Human Feedback) systems, helping align model outputs with human expectations. This ensures that the models can generate more accurate and relevant financial advice. The dataset is a valuable resource for refining financial language models and aligning them with user needs and legal regulations.","RLHF, Finance, QA, Evaluation",Prompt and response,35.8,"13,269",29/07/2023,English
https://huggingface.co/datasets/trl-lib/lm-human-preferences-sentiment?row=0,'LM Human Preferences Sentiment (TRL-Lib)',"The LM Human Preferences Sentiment dataset focuses on refining the quality of text-based sentiment predictions by leveraging human evaluations. It presents prompts followed by two distinct responses: one chosen by human evaluators (the preferred response) and one rejected. These comparisons are essential for tuning large language models to align more closely with human sentiment expectations and improve their capacity to generate meaningful, context-aware emotional responses. The dataset is structured to address nuanced text generation tasks, helping language models discern subtle emotional cues and ensure accurate sentiment detection. Evaluators assess multiple layers of sentiment to select the response that best aligns with the intended emotional tone, enhancing the relevance and accuracy of generated text. This dataset plays a crucial role in RLHF (Reinforcement Learning with Human Feedback) by refining language models for applications involving narrative understanding, emotional intelligence, and conversational agents.","RLHF, Sentiment Analysis, Evaluation",Accepted/rejected,1.6,"6,264",11/06/2023,English
https://huggingface.co/datasets/yankihue/h_positive_tweets_human_feedback,'Positive Tweets Human Feedback (Yankihue)',"The H Positive Tweets Human Feedback dataset focuses on the evaluation of positive sentiment in tweets through human feedback. This dataset consists of prompts followed by two responses—one preferred (chosen) and the other rejected—based on evaluations provided by human annotators. The goal is to fine-tune sentiment models by training them to better capture positive expressions, helping language models accurately identify and generate uplifting content. The dataset plays a role in refining sentiment analysis systems by incorporating human judgment into model training, thereby addressing nuances that automated systems may overlook. It is designed for applications that require the detection of positive language, making it valuable for social media sentiment tracking, customer support, and chatbot interactions.","Sentiment Analysis, Social Media, NLP",Accepted/rejected,0.322,"2,000",05/12/2022,Turkish
https://huggingface.co/datasets/allenai/preference-test-sets,'Preference Test Sets (Allen AI)',"The ""Preference Test Sets"" dataset, curated by AllenAI, gathers various preference datasets into a unified schema for consistent loading and validation. These datasets enable the evaluation of reward models, with subsets including data from Anthropic’s Helpful & Harmless (HH) Agent, PKU SafeRLHF, and Stanford Human Preferences. Additionally, the collection supports benchmarking tasks such as summarization and question answering, offering both human and GPT-4 evaluations for deeper insights into reward model biases. The dataset was initially updated 7 months ago, reflecting a careful alignment with contemporary alignment and RLHF evaluation needs.","RLHF, Evaluation, Benchmarking",Accepted/rejected,26.7,"43,175",01/01/2024,English
https://huggingface.co/datasets/YiyangAiLab/POVID_preference_data_for_VLLMs,'POVID Preference Data for VLLMs (YiyangAiLab)',"The POVID preference data for VLLMs dataset offers visual-language data focusing on preference comparisons for multimodal models. It features conversations that evaluate visual reasoning capabilities, including detailed image descriptions and contextual image-based interactions. Each entry typically includes paired responses, with one marked as chosen and the other as rejected based on alignment with the prompt. The images referenced in the dataset are sourced from COCO, ensuring that the conversations revolve around common objects and scenarios depicted in the dataset. The dataset serves as a resource for refining and validating visual-language models (VLLMs), focusing on how well models can interpret, describe, or answer questions about images. This resource aids in the development of robust VLLMs by providing comparisons of responses to visual prompts, thus improving future model fine-tuning through preference-based reinforcement learning techniques.","Visual QA, Dialogue, Preference Learning",Accepted/rejected,20.9,"17,184",10/01/2023,English
https://huggingface.co/datasets/OpenAssistant/oasst1,'OpenAssistant OASST1 (Open Assistant)',"The OpenAssistant Conversations Dataset (OASST1) is a multilingual collection of 161,443 messages spanning 35 languages. It was created to support research in the alignment of large language models (LLMs). Compiled from a global effort involving over 13,500 volunteers, the dataset includes over 10,000 conversation trees and 461,292 quality ratings. Each message tree begins with a prompt and alternates between roles of ""prompter"" and ""assistant."" It covers a range of topics and languages, with English being the most represented. Subsets include ready-for-export trees, complete collections with moderated content, and spam-filtered outputs, all designed for fine-tuning and reward model evaluation. The dataset is publicly accessible on Hugging Face and GitHub, accompanied by tools for analysis.","Dialogue, Multilingual, Evaluation",Accepted/rejected,41.6,"88,838",12/04/2023,English
https://huggingface.co/datasets/defunct-datasets/eli5,'ELI5 (Defunct Datasets)',"The ELI5 dataset is a collection of questions and answers from the Explain Like I’m Five (ELI5), AskScience, and AskHistorians subreddits. This dataset focuses on supporting open-domain long-form question answering (LFQA) by providing detailed, community-generated responses to user queries. It contains a structured set of data, including questions, answers, and related metadata, to train models in retrieving and generating coherent, multi-sentence responses. The dataset is divided into training, validation, and test sets across the three subreddits, covering diverse topics from general knowledge to history and science. ELI5 emphasizes readability and accessibility, making it suitable for models aimed at layperson-friendly explanations.","NLG, QA, Reddit, Educational",,,"550,000",28/07/2019,English
https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences,'Stack Exchange Preferences (HuggingFaceH4)',"The H4 Stack Exchange Preferences Dataset consists of curated questions and answers from the Stack Overflow Data Dump. This dataset supports preference model pretraining (PMP) by focusing on questions with at least two answers, applying a scoring method aligned with the Anthropic model. The dataset is structured to facilitate instruction fine-tuning, language model training, and preference model pretraining through pairwise comparisons, as outlined in Askell et al. (2021). It includes scripting tools to help create binary datasets from the ranked pairs. This English-only dataset is intended for model training, though some non-English content may appear from excluded Stack Exchange communities.","Preference Learning, Instruction, QA",Prompt and response,"19,700","10,807,695",12/01/2016,English
https://huggingface.co/datasets/nomic-ai/gpt4all_prompt_generations,'GPT4All Prompt Generations (Nomic AI)',"The GPT4All Prompt Generations dataset, curated by Nomic AI, is designed to train and evaluate large language models like GPT-4All. It consists of prompt-response pairs sourced from repositories such as pacovaldez/stackoverflow-questions. The prompts include technical queries, such as those about programming in SparkSQL or mnesia, with the corresponding responses offering solutions or advice, primarily related to coding practices and error handling. This dataset supports language model fine-tuning and instruction-based evaluation by providing real-world use cases derived from Stack Overflow-style exchanges, making it valuable for developing more accurate and practical conversational AI models.","Instruction, NLG, QA, Technical Queries",Prompt and response,398,"437,604",12/01/2016,English
https://huggingface.co/datasets/maywell/hh-rlhf-nosafe,'HH RLHF NoSafe (Maywell)',"The HH-RLHF dataset, derived from the work of Anthropic, focuses on reward signals without applying safety constraints during its creation. It encompasses two primary data types: human preference data related to helpfulness and harmlessness, designed for training preference or reward models, and human-generated red teaming dialogues aimed at understanding how crowdworkers challenge language models. The human preference data includes pairs of texts labeled as ""chosen"" and ""rejected,"" structured to facilitate reinforcement learning from human feedback. In contrast, the red teaming dialogues provide full transcripts of interactions between red team members and AI assistants, annotated with scores for harmlessness and descriptions of red team attempts. The dataset highlights the need for caution, as it contains potentially offensive content, making it unsuitable for training dialogue agents without risk of harmful outcomes.","RLHF, Harmless, Dialogue, Red Teaming",Accepted/rejected,65.3,"124,503",12/04/2023,English
https://www.kaggle.com/datasets/lizhecheng/rlhf-dataset,'RLHF Dataset (Lizhe Cheng)',"The RLHF Dataset is a resource designed for reinforcement learning from human feedback, specifically tailored for training language models. This dataset comprises a collection of dialogues that can be used to improve the performance of AI models in understanding and generating human-like responses. The dialogues included are meant to facilitate the development of systems that can learn from human preferences, making them more effective in tasks that require nuanced understanding and engagement.","RLHF, Dialogue, NLG",,26.97,,11/1/2023,English
https://github.com/google-research-datasets/richhf-18k,'RICH HF 18K (Google Research)',"The RichHF-18K dataset includes 18,000 human feedback samples, featuring subjective scores and human-labeled heatmaps for evaluating language models, particularly in text-to-image generation tasks.","Human Feedback, Text-to-Image, NLG",,,"18,000",01/12/2023,English
https://huggingface.co/datasets/openbmb/UltraFeedback,'Ultra Feedback (OpenBMB)',"UltraFeedback is a large-scale dataset comprising approximately 64,000 prompts and 256,000 responses, designed for fine-tuning large language models by collecting diverse human feedback. The dataset is annotated based on four key aspects: instruction-following, truthfulness, honesty, and helpfulness, using multiple state-of-the-art language models for generating responses. It aims to enhance task-specific performance in reinforcement learning from human feedback (RLHF).","RLHF, NLG, Instruction, Multilingual",Classification,940,"63,967",01/10/2023,English
https://huggingface.co/datasets/lvwerra/stack-exchange-paired,'Stack Exchange Paired (Lvwerra)',"This dataset is a processed version of the HuggingFaceH4/stack-exchange-preferences, designed for preference learning. It consists of paired responses from Stack Exchange questions where one response is rated better than the other. The data undergoes several processing steps, including parsing HTML to Markdown, creating pairs from responses, and shuffling the dataset to ensure variety. It provides valuable insights for fine-tuning models based on human preferences regarding model-generated responses.","Preference Learning, Instruction, QA",Prompt and response,"26,300","31,284,837",20/05/2011,English
https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned,'Ultra Feedback Binarized Preferences Cleaned (Argilla)',"This dataset is a refined version of the UltraFeedback dataset, featuring binarized human preferences on language model outputs for task-specific feedback. It incorporates a new methodology for binarization based on the average of preference ratings, addressing issues related to contamination from the TruthfulQA dataset. The dataset is formatted to align with various Hugging Face resources, enhancing its usability for fine-tuning language models. Argilla encourages researchers to explore and experiment with this dataset to improve language model training.","RLHF, NLG, Feedback, Instruction",Accepted/rejected,143,"60,917",14/03/2024,English
https://huggingface.co/datasets/jondurbin/truthy-dpo-v0.1,'Truthy DPO V0.1 (Jon Durbin)',"The Truthy DPO dataset is designed to improve the truthfulness of language models while maintaining the immersive experience of roleplaying as a human. It focuses on enhancing models' understanding of corporeal, spatial, and temporal awareness, as well as addressing common misconceptions. The dataset is particularly useful for training dialogue models to generate more accurate and truthful statements without losing the human-like quality of interactions. Contributions for new functionalities and datasets are welcomed, and support for fine-tuning costs is encouraged through various means.","DPO, Truthfulness, Dialogue",Accepted/rejected,0.653,"1,016",14/01/2024,English
https://huggingface.co/datasets/m-a-p/Code-Feedback,'Code Feedback (M-A-P)',"The OpenCodeInterpreter dataset consists of prompts and responses generated by models like GPT-4 and GPT-3.5. It serves as a bridge between large language models and advanced proprietary systems, enhancing code generation capabilities by integrating execution and iterative refinement functionalities. The dataset includes various programming tasks and provides examples of user queries along with AI-generated code solutions, showcasing the models' ability to handle complex coding challenges.","Coding, Instruction, NLG",,413,"66,383",14/02/2024,English
https://huggingface.co/datasets/wenbopan/Chinese-dpo-pairs,'Chinese DPO Pairs (Wenbo Pan)',"This dataset consists of well-curated 10,000 dialogue pairs in Chinese, designed to improve conversational accuracy and relevance in language models. The data was generated by translating from multiple sources, including flan_v2, sharegpt, and ultrachat, among others. The dataset aims to enhance training methodologies for models to respond more accurately in Chinese conversations.","Chinese, Dialogue, NLG",Accepted/rejected,17.4,"10,735",01/11/2023,Chinese
https://huggingface.co/datasets/b-mc2/sql-create-context,'SQL Create Context (B-Mc2)',"This dataset features 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL queries that utilize the CREATE statement as context. It is designed for text-to-SQL models, aiming to prevent hallucination of column and table names during training. Built from WikiSQL and Spider, the dataset underwent cleansing and augmentation to enhance its quality and usability for LLMs.","SQL, NLG, QA",Prompt and response,21.8,"78,577",12/10/2023,English
https://huggingface.co/datasets/eswardivi/telugu_instruction_dataset,'Telugu Instruction Dataset (Eswar Divi)'," A dataset aimed at training models for multilingual tasks, featuring instructions and responses in the Telugu language, designed to enhance conversational understanding in that language.","Multilingual, Telugu, Dialogue",Prompt and response,78.4,"145,048",14/01/2024,Telugu
https://huggingface.co/datasets/wzhouad/zephyr-ultrafeedback-hybrid,'Zephyr Ultra Feedback Hybrid (Wzhouad)',"A dataset focused on feedback collected from various sources to enhance language models, blending both structured feedback and model outputs.","Human Feedback, NLG",Accepted/rejected,178,"64,680",01/10/2023,English
https://huggingface.co/datasets/wzhouad/gemma-2-ultrafeedback-hybrid,'Gemma-2 Ultra Feedback Hybrid (Wzhouad)',"Provides multi-faceted feedback data aimed at training and evaluating advanced models, supporting preference-based tuning.","Human Feedback, Evaluation",Accepted/rejected,194,"61,599",01/10/2023,English
https://huggingface.co/datasets/wzhouad/llama3-ultrafeedback-hybrid,'Llama3 Ultra Feedback Hybrid (Wzhouad)', A hybrid feedback dataset designed to fine-tune language models with a mix of user preferences and critical annotations.,"Human Feedback, Instruction",Accepted/rejected,189,"64,480",01/10/2023,English
https://huggingface.co/datasets/wzhouad/llama3-ultrafeedback-hybrid-v2,'Llama3 Ultra Feedback Hybrid V2 (Wzhouad)',An updated version offering refined feedback to improve the performance of large language models.,"Feedback, NLG, RLHF",Accepted/rejected,186,"64,479",02/10/2023,English
https://www.kaggle.com/datasets/thedevastator/anthropic-helpfulness-harmlessness-preference-da,'Anthropic Helpfulness Harmlessness Preference (The Devastator)',"This dataset, hosted on Kaggle, contains human preference data used for RLHF, evaluating models for helpfulness and harmlessness.","RLHF, Evaluation, Harmless",Accepted/rejected,324.93,"8,552",,English
https://huggingface.co/datasets/NinaCalvi/lfqa_expert_pairwise_human_preference_no_reasoning?,'LFQA Expert Pairwise Human Preference No Reasoning (Nina Calvi)',Contains pairwise expert judgments for long-form question answering without detailed reasoning.,"QA, Expert Feedback, NLG",Prompt and response,0.444,260,07/07/2023,English
https://huggingface.co/datasets/DatPySci/HH-RLHF-preprocessed,'HH RLHF Preprocessed (DatPySci)'," A preprocessed version of the Helpful-Harmless RLHF dataset, ready for model fine-tuning and training.","RLHF, Harmless, Training",Accepted/rejected,120,"119,459",25/08/2023,English
https://huggingface.co/datasets/omnineura/EE_QA_for_RLHF,'EE QA for RLHF (Omnineura)',A dataset designed to enhance question-answering models with human feedback for RLHF research.,"RLHF, QA, NLG",Prompt and response,0.225,102,12/06/2023,English
https://huggingface.co/datasets/Fizzarolli/hh-rlhf-helpful-only,'HH RLHF Helpful Only (Fizzarolli)',"Focused solely on helpfulness, this subset of the RLHF dataset is curated to fine-tune models on assistance-related tasks.","RLHF, Helpfulness, Dialogue",Accepted/rejected,146,"118,263",01/09/2023,English
https://huggingface.co/datasets/cm2435cm2435/rlhf-search-rerank,'RLHF Search Rerank (Cm2435cm2435)',A dataset for improving search result relevance using RLHF principles through preference-based ranking models.,"RLHF, Search, QA",Prompt and response,7.19,"8,552",20/08/2023,English
https://huggingface.co/datasets/IlyaGusev/rulm_human_preferences?,'RULM Human Preferences (Ilya Gusev)',"This dataset captures user preferences to fine-tune Russian language models, aiming for more relevant and engaging interactions.","RLHF, Russian, Dialogue",Accepted/rejected,12.7,"34,520",17/09/2023,Russian
